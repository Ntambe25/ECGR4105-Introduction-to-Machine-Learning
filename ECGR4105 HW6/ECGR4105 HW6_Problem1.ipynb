{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2308c64d",
   "metadata": {},
   "source": [
    "## ECGR 4105 - Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00014",
   "metadata": {},
   "source": [
    "## Problem 1, Part a\n",
    "\n",
    "For 1st Part, a Fully Connected Neural Network for all 10 classes in CIFAR-10 will be created with only one hidden layer with the size of 512. The Network will be trained for 300 epochs. Training Time, Training Loss, and Evaluation Accuracy after 300 epochs will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adf4bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing Pytorch and Tensorflow\n",
    "import torch\n",
    "torch.version.__version__\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12644b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Importing/Downloading the CIFAR-10 DataSet\n",
    "\n",
    "from torchvision import datasets\n",
    "data_path = \"../data-unversioned/p1ch7/\"\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabe3b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  50000\n",
      "Validation Set:  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set: \", len(cifar10))\n",
    "print(\"Validation Set: \", len(cifar10_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c259b7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ../data-unversioned/p1ch7/\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../data-unversioned/p1ch7/\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "print(cifar10)\n",
    "print(cifar10_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2bd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertion of the PIL Images into PyTorch Tensors\n",
    "from torchvision import transforms\n",
    "\n",
    "tensor_cifar10 = datasets.CIFAR10(data_path, train = True, download = False,\n",
    "                                  transform = transforms.ToTensor())\n",
    "\n",
    "tensor_cifar10_val = datasets.CIFAR10(data_path, train = False, download = False,\n",
    "                                      transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fd6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n",
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim = 3)\n",
    "imgs.shape\n",
    "\n",
    "print(imgs.view(3, -1).mean(dim = 1))\n",
    "print(imgs.view(3, -1).std(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9da3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4942, 0.4851, 0.4504])\n",
      "tensor([0.2467, 0.2429, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "imgs_val = torch.stack([img_t_val for img_t_val, _ in tensor_cifar10_val], dim = 3)\n",
    "imgs_val.shape\n",
    "\n",
    "print(imgs_val.view(3, -1).mean(dim = 1))\n",
    "print(imgs_val.view(3, -1).std(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39b1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(data_path, train = True, download = False,\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2470, 0.2435, 0.2616))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8463a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10_val = datasets.CIFAR10(data_path, train = False, download = False,\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4942, 0.4851, 0.4404),\n",
    "                         (0.2467, 0.2429, 0.2616))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d958621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing 'datetime' Module included with Python\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa3070dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-08 08:02:46.473520 Epoch 0, Training Loss 1.781964898109436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader_part_a:\n\u001b[0;32m     18\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model_part_a\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_Fn(outputs, labels)\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the Model (with only 1 Hidden Layer)\n",
    "\n",
    "train_loader_part_a = torch.utils.data.DataLoader(transformed_cifar10, batch_size = 128, shuffle = True)\n",
    "\n",
    "loaded_model_part_a = nn.Sequential(nn.Linear(3072, 512), nn.Tanh(),\n",
    "                      nn.Linear(512, 10),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(loaded_model_part_a.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_Fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader_part_a:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = loaded_model_part_a(imgs.view(batch_size, -1))\n",
    "        loss = loss_Fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if(epoch % 10 == 0):\n",
    "        print(\"{} Epoch {}, Training Loss {}\".format(datetime.datetime.now(), epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daecaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_part_a.state_dict(), data_path + \"Problem1_PartA.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e53eef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4195\n"
     ]
    }
   ],
   "source": [
    "# Computing the Accuracy of the Model using the Validation Set \n",
    "\n",
    "val_loader_part_a = torch.utils.data.DataLoader(transformed_cifar10_val, batch_size = 128, shuffle = False)\n",
    "\n",
    "# Initializing the values of the Variables \"correct\"\" and \"total\" for further use\n",
    "correct = 0;\n",
    "total = 0;\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs_val, labels in val_loader_part_a:\n",
    "        batch_size = imgs_val.shape[0]\n",
    "        outputs = loaded_model_part_a(imgs_val.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim = 1)\n",
    "        \n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: \", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5fe5ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model_part_a.state_dict(), data_path + \"Problem1_PartA.pt\")\n",
    "loaded_model_part_a = nn.Sequential(nn.Linear(3072, 512), nn.Tanh(),\n",
    "                      nn.Linear(512, 10),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "loaded_model_part_a.load_state_dict(torch.load(data_path + \"Problem1_PartA.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269539d",
   "metadata": {},
   "source": [
    "## Problem 1, Part b\n",
    "\n",
    "For 2nd Part, the Network will be extended with two more additional hidden layers. The Network will be trained for 300 epochs and the Training Time, Training Loss, and Evaluation Accuracy after 300 epochs will be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447c94b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-08 01:41:47.258438 Epoch 0, Training Loss 2.0479979515075684\n",
      "2022-12-08 01:50:34.266069 Epoch 10, Training Loss 1.6230140924453735\n",
      "2022-12-08 01:59:16.812208 Epoch 20, Training Loss 1.342703104019165\n",
      "2022-12-08 02:07:58.768081 Epoch 30, Training Loss 1.2078759670257568\n",
      "2022-12-08 02:16:43.894495 Epoch 40, Training Loss 1.450156807899475\n",
      "2022-12-08 02:25:26.807374 Epoch 50, Training Loss 0.9796058535575867\n",
      "2022-12-08 02:34:10.600114 Epoch 60, Training Loss 0.9214929938316345\n",
      "2022-12-08 02:42:56.486086 Epoch 70, Training Loss 1.1126482486724854\n",
      "2022-12-08 02:51:40.032863 Epoch 80, Training Loss 1.0062534809112549\n",
      "2022-12-08 03:00:21.689096 Epoch 90, Training Loss 0.9504820108413696\n",
      "2022-12-08 03:09:02.445550 Epoch 100, Training Loss 1.1079751253128052\n",
      "2022-12-08 03:17:43.685620 Epoch 110, Training Loss 0.8577156662940979\n",
      "2022-12-08 03:26:23.536683 Epoch 120, Training Loss 1.0073959827423096\n",
      "2022-12-08 03:35:04.459506 Epoch 130, Training Loss 0.9235573410987854\n",
      "2022-12-08 03:43:54.980475 Epoch 140, Training Loss 0.8181724548339844\n",
      "2022-12-08 03:52:43.729013 Epoch 150, Training Loss 0.9429395198822021\n",
      "2022-12-08 04:01:28.837981 Epoch 160, Training Loss 0.7974256873130798\n",
      "2022-12-08 04:10:14.921705 Epoch 170, Training Loss 0.8919129371643066\n",
      "2022-12-08 04:18:58.860586 Epoch 180, Training Loss 0.7974254488945007\n",
      "2022-12-08 04:27:41.586756 Epoch 190, Training Loss 0.8912424445152283\n",
      "2022-12-08 04:36:23.237821 Epoch 200, Training Loss 1.0799740552902222\n",
      "2022-12-08 04:45:11.951338 Epoch 210, Training Loss 0.8911986947059631\n",
      "2022-12-08 04:53:52.354970 Epoch 220, Training Loss 0.9117913246154785\n",
      "2022-12-08 05:02:33.986244 Epoch 230, Training Loss 0.9219064116477966\n",
      "2022-12-08 05:11:30.787215 Epoch 240, Training Loss 0.8911435008049011\n",
      "2022-12-08 05:20:16.486450 Epoch 250, Training Loss 0.7970902919769287\n",
      "2022-12-08 05:28:58.188048 Epoch 260, Training Loss 1.005816102027893\n",
      "2022-12-08 05:37:38.178341 Epoch 270, Training Loss 0.7971267700195312\n",
      "2022-12-08 05:46:21.670988 Epoch 280, Training Loss 0.8910205364227295\n",
      "2022-12-08 05:55:05.840664 Epoch 290, Training Loss 0.8910744786262512\n"
     ]
    }
   ],
   "source": [
    "# Training the Model (with 2 Additional Hidden Layer)\n",
    "\n",
    "train_loader_part_b = torch.utils.data.DataLoader(transformed_cifar10, batch_size = 64, shuffle = True)\n",
    "\n",
    "model_part_b = nn.Sequential(nn.Linear(3072, 2048), nn.Tanh(),\n",
    "                      nn.Linear(2048, 512), nn.Tanh(),\n",
    "                      nn.Linear(512, 256), nn.Tanh(),\n",
    "                      nn.Linear(256, 10), nn.Tanh(),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_part_b.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_Fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 300\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader_part_b:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_part_b(imgs.view(batch_size, -1))\n",
    "        loss = loss_Fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if(epoch % 10 == 0):\n",
    "       print(\"{} Epoch {}, Training Loss {}\".format(datetime.datetime.now(), epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56c2f42d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 3072]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36mNet_B.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)))\n\u001b[0;32m     17\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\ECGR4105\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 3072]"
     ]
    }
   ],
   "source": [
    "# Computing the Accuracy of the using the Validation Set \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(transformed_cifar10_val, batch_size = 128, shuffle = False)\n",
    "\n",
    "# Initializing the values of the Variables \"correct\"\" and \"total\" for further use\n",
    "correct = 0;\n",
    "total = 0;\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim = 1)\n",
    "        \n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ffd0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_part_b.state_dict(), data_path + \"Problem1_PartB.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
